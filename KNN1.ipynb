{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b65c73c-82a0-48ea-9b03-b9b44015285d",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5399568-3ada-499f-b45f-974dc0bfacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for classification and regression tasks. \n",
    "It assigns a data point to the majority class or predicts a numerical value based on the classes of its k-nearest neighbors \n",
    "in the feature space.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b40299-2642-4d4c-b68a-11d0bd3373d0",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755d342-67ee-42a6-9615-903f82f2916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Choosing the value of K in KNN is crucial for the algorithm's performance. A small K may lead to noise sensitivity,\n",
    "while a large K might result in over-smoothing. Common methods for selecting K include:\n",
    "1. Cross-Validation: Use techniques like k-fold cross-validation to evaluate KNN's performance for different K values and choose the one with the best accuracy.\n",
    "2. Grid Search: Systematically test multiple K values within a specified range and select the one optimizing the model's performance metric.\n",
    "3. Domain Knowledge: Consider the nature of the problem and the dataset. A meaningful K could align with the underlying patterns in the data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cec78-f224-469e-b5e9-07f733dd0458",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96d6c1-fce8-4886-a722-981a9da55345",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The main difference between KNN classifier and KNN regressor lies in their respective tasks:\n",
    "\n",
    "1. KNN Classifier:\n",
    "   - Task: Used for classification problems where the goal is to assign a data point to a specific category or class.\n",
    "   - Output: Provides the class label or category for the input data point based on the majority class of its k-nearest neighbors.\n",
    "\n",
    "2. KNN Regressor:\n",
    "   - Task: Applied to regression problems, aiming to predict a numerical or continuous output.\n",
    "   - Output: Provides a numerical value as the prediction, typically the mean or weighted mean of the target values of its k-nearest neighbors.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9fd763-cf8e-4d8d-bc4b-1cfa9b61576a",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a4154-a15f-4d5d-8336-cfadb5847be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The performance of k-nearest neighbors (KNN) is commonly evaluated using metrics such as accuracy, precision, recall, and F1 \n",
    "score. Accuracy represents the proportion of correctly classified instances among all instances. Precision measures the accuracy\n",
    "of positive predictions, while recall assesses the model's ability to capture all positive instances. The F1 score provides a\n",
    "balance between precision and recall. Additionally, cross-validation techniques, such as k-fold cross-validation, help assess\n",
    "model robustness by splitting the dataset into multiple folds for training and testing. A low k-value in KNN may lead to overfitting, \n",
    "while a high k-value can result in underfitting. Therefore, tuning the k-value based on performance metrics is essential. \n",
    "These evaluations provide insights into the effectiveness of KNN in handling different datasets and guide parameter selection\n",
    "for optimal model performance.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978b31e-0a9e-43bb-b6ef-1a0ee3783208",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05dfaf-daaa-49f6-b322-d47aee2bacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The curse of dimensionality in K-nearest neighbors (KNN) refers to the challenges and deteriorating performance that arise as\n",
    "the number of features or dimensions increases. In high-dimensional spaces, the distance between data points becomes less \n",
    "meaningful, as most points are equidistant from each other. Consequently, the concept of proximity, which is fundamental to \n",
    "KNN, loses its discriminatory power, leading to increased computational complexity and decreased predictive accuracy. This\n",
    "phenomenon results in a sparsity of data and necessitates larger datasets to maintain the effectiveness of the algorithm.\n",
    "Dimensionality reduction techniques or careful feature selection are often employed to mitigate the curse of dimensionality in\n",
    "KNN and enhance its performance in high-dimensional spaces.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c585f-a164-470e-a63c-d97fb99d1c50",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb49b07-5d7a-4cba-ab38-646e7eaacaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Handling missing values in k-nearest neighbors (KNN) involves imputing or predicting the missing values based on the values \n",
    "of neighboring instances. One common approach is to replace missing values with the mean, median, or mode of the feature across\n",
    "the dataset. However, in the context of KNN, a more sophisticated method is to predict missing values by considering the values\n",
    "of the nearest neighbors.\n",
    "\n",
    "\n",
    "\n",
    "1. Identify missing values: Locate and mark the missing values in the dataset.\n",
    "\n",
    "2. Neighbor selection: For each instance with missing values, identify its k-nearest neighbors based on the available features.\n",
    "\n",
    "3. Imputation: Average or use the majority vote of the corresponding feature values from the nearest neighbors to impute the missing value.\n",
    "\n",
    "4. Repeat: Iterate through the dataset, identifying instances with missing values, and impute them using their respective neighbors.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8a3f2-4bdc-4c9a-90a8-df1485e4982c",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a5d59-bf0e-4878-9989-0dff804908f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "K-nearest neighbors (KNN) classifier and regressor differ in their output and application. The KNN classifier predicts the\n",
    "class of a data point based on the majority class of its k-nearest neighbors, while the KNN regressor predicts a continuous \n",
    "output by averaging the values of its k-nearest neighbors.\n",
    "\n",
    "In classification tasks, where the goal is to categorize data into predefined classes (e.g., spam or not spam), the KNN classifier\n",
    "is suitable. It is effective when decision boundaries are complex and not easily captured by simple models.\n",
    "\n",
    "For regression tasks, where the objective is to predict a numeric value (e.g., house prices), the KNN regressor is appropriate. \n",
    "It is useful when the relationship between features and the target variable is non-linear and lacks a clear mathematical expression.\n",
    "\n",
    "The choice between KNN classifier and regressor depends on the nature of the problemâ€”classify for categorical outcomes and \n",
    "regress for continuous predictions. The performance of each should be assessed based on metrics relevant to the specific task,\n",
    "such as accuracy for classification and mean squared error for regression.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0a6cd-d140-42fb-9f61-e940ca54e002",
   "metadata": {},
   "source": [
    "# Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224a584-c5e6-41ad-b7df-d3a505ebf035",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Strengths of KNN:\n",
    "1. Simplicity: KNN is intuitive and easy to understand.\n",
    "2. Non-parametric: It makes no assumptions about the underlying data distribution.\n",
    "3. Adaptability: Suitable for various types of data and can handle non-linear relationships.\n",
    "\n",
    "Weaknesses of KNN:\n",
    "1. Computational cost: Can be computationally expensive, especially in high-dimensional spaces.\n",
    "2. Sensitivity to outliers: Outliers can significantly impact predictions.\n",
    "3. Curse of dimensionality: Performance deteriorates as the number of features increases.\n",
    "\n",
    "Addressing Weaknesses:\n",
    "1. Dimensionality Reduction: Use techniques like PCA to reduce the number of features.\n",
    "2. Normalization: Scale features to address sensitivity to varying magnitudes.\n",
    "3. Outlier Handling: Robustify KNN by using distance metrics less sensitive to outliers or employing outlier detection techniques.\n",
    "4. Optimal k-value: Experiment with different k-values through cross-validation to find the most suitable for the dataset.\n",
    "5. Lazy Learning: Consider approximations or hybrid models to improve computational efficiency without compromising accuracy.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ab257-fca3-481e-a277-5b5d8f42a218",
   "metadata": {},
   "source": [
    "# Ans : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827fa68-a61b-4516-a3d5-a16cd9809552",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Euclidean distance and Manhattan distance are distance metrics used in K-nearest neighbors (KNN) algorithm, measuring the similarity between data points.\n",
    "\n",
    "Euclidean Distance:\n",
    "- Calculates the straight-line distance between two points in a multidimensional space.\n",
    "- Given two points (x1, y1) and (x2, y2), Euclidean distance is the square root of the sum of squared differences: sqrt((x2 - x1)^2 + (y2 - y1)^2).\n",
    "- Reflects geometric proximity and is sensitive to magnitude and direction.\n",
    "\n",
    "Manhattan Distance:\n",
    "- Also known as L1 distance or city block distance.\n",
    "- Measures the distance between two points by summing the absolute differences along each dimension.\n",
    "- For points (x1, y1) and (x2, y2), Manhattan distance is |x2 - x1| + |y2 - y1|.\n",
    "- Ignores diagonal distances and is less sensitive to outliers or variations in magnitude.\n",
    "\n",
    "In KNN, the choice between Euclidean and Manhattan distance depends on the nature of the data and the importance of each \n",
    "dimension in determining similarity. Euclidean distance is suitable for scenarios where direction and magnitude matter, while\n",
    "Manhattan distance is useful when only the magnitude matters, and dimensions are independent.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe10dd-5bd7-437a-a8e7-8d7198290fa7",
   "metadata": {},
   "source": [
    "# Ans : 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2b003-1537-46a4-87f9-d98f02072f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
